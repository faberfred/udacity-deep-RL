{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE-improvements\n",
    "* Noise reduction\n",
    "* Reward Normalization\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train REINFORCE with OpenAI Gym's Cartpole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/horst/anaconda3/envs/drlnd-4-copy/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "env.seed(0)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "#     def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "    def __init__(self, h_size=16):\n",
    "        super(Policy, self).__init__()\n",
    "        s_size = env.observation_space.shape[0]\n",
    "#         a_size = env.action_space.shape[0]  # for continuous action spaces\n",
    "        a_size = env.action_space.n    # for discrete action spaces\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # return a tensor containing the probabilities of each action\n",
    "        return F.softmax(x, dim=1)\n",
    "#         return x.cpu().data\n",
    "\n",
    "    \n",
    "    def act(self, state):\n",
    "        # transform the state to a torch tensor and send it to device (GPU or CPU) \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        # compute the action probabilities -> do the rest of the calculation on CPU\n",
    "        probs = self.forward(state).cpu()\n",
    "        # Create a categorical distribution parameterized by probs\n",
    "        m = Categorical(probs)\n",
    "        # sample according to the underlying probability\n",
    "        action = m.sample()\n",
    "        # return the action item and the log of the underlying probability!\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with the improved REINFORCE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 66.49\n",
      "Episode 200\tAverage Score: 79.61\n",
      "Episode 300\tAverage Score: 192.98\n",
      "Environment solved in 302 episodes!\tAverage Score: 196.22\n"
     ]
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "def reinforce(n_episodes=3000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    # save the last 100 scores within a double-ended-queue \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # run n_trajectories trajectories\n",
    "        n_trajectories = 5\n",
    "        trajectory_reward = []\n",
    "        trajectory_log_probs = []\n",
    "        trajectory_loss = []\n",
    "        reward_mu = 0\n",
    "        reward_sigma = 0\n",
    "        \n",
    "        # collect n_trajectories trajectories\n",
    "        for i_trajectory in range(n_trajectories):\n",
    "            saved_log_probs = []\n",
    "            rewards = []\n",
    "            # reset the state\n",
    "            state = env.reset()\n",
    "            for t in range(max_t):\n",
    "                # get the next action (item) and the log of its underlying probability\n",
    "                action, log_prob = policy.act(state)\n",
    "                # store the log of the underlying probability!\n",
    "                saved_log_probs.append(log_prob)\n",
    "                # do the next step in the environment according to the action\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                # store the reward\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    break\n",
    "            # sum up the rewards of the trajectory and store it in the double-ended-queue\n",
    "            scores_deque.append(sum(rewards))\n",
    "             # sum up the rewards of the trajectory and store it\n",
    "            scores.append(sum(rewards))\n",
    "\n",
    "            # calculate the discount factor\n",
    "            discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "            # cumpute the total reward of the trajectory (discount * reword) = R(tau^i_episode)\n",
    "            R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "            \n",
    "            # store the reward of the trajectory\n",
    "            trajectory_reward.append(R)\n",
    "            # store the log probabilities of each time step of the trajectory\n",
    "            trajectory_log_probs.append(saved_log_probs)\n",
    "        \n",
    "        # compute the normalized reward\n",
    "        reward_mu = sum(trajectory_reward) / n_trajectories\n",
    "        \n",
    "        tt = []\n",
    "        for r in trajectory_reward:\n",
    "            tt.append((r - reward_mu)**2)\n",
    "            \n",
    "        reward_sigma = np.sqrt(sum(tt) / n_trajectories)\n",
    "#         if(reward_sigma == 0):\n",
    "#             print(reward_sigma)\n",
    "        \n",
    "        # compute the normalized reward\n",
    "        R_normalized = []\n",
    "        for r in trajectory_reward:\n",
    "            if reward_sigma > 0:\n",
    "                R_normalized.append((r-reward_mu) / reward_sigma)\n",
    "            else:\n",
    "                R_normalized.append(reward_mu)\n",
    "        \n",
    "        # compute the policy-loss \n",
    "        for i, log_probs in enumerate(trajectory_log_probs):\n",
    "            policy_loss = []\n",
    "            for log_prob in log_probs:\n",
    "                policy_loss.append(-log_prob * R_normalized[i])\n",
    "            trajectory_loss.append(torch.cat(policy_loss).sum())\n",
    "        \n",
    "        trajectory_policy_loss = sum(trajectory_loss) / n_trajectories        \n",
    "\n",
    "        # reset the gradints to zero (because they are accumulated)\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradiants = ĝ => U(theta)\n",
    "        trajectory_policy_loss.backward()\n",
    "        # do the weight updates => theta = theta + lr * ĝ\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(1000):\n",
    "    action, _ = policy.act(state)\n",
    "    env.render()\n",
    "    time.sleep(0.02)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
